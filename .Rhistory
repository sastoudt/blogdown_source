meanLambda=mean( exp(x[1]+x[2]*z))
meanP=mean( exp(x[3]+x[4]*m)/(1+exp(x[3]+x[4]*m)))
return(list(meanLambda=meanLambda,meanP=meanP))
}
getMeans=apply(coeffEst,1,getMeanPred)
getMeanLambda=lapply(getMeans,function(x){x$meanLambda})
getMeanP=lapply(getMeans,function(x){x$meanP})
trueAbun=unlist(lapply(alpha,function(x){x*mean(lambda)}))
trueP=unlist(lapply(alpha,function(x){1/x*mean(p)}))
plot(trueAbun,getMeanLambda)
summary(models[[20]])
coeffEst
summary(y[[1-]])
summary(y[[10]])
summary(y[[20]])
summary(y[[50]])
summary(y[[5]])
set.seed(217117)
n=1000
z<- runif(n, -2.5,2.5) # simulate a covariate for occurrence
m<-runif(n,-1,1) # simulate a covariate for detection
llambda<- 0.5 +0.5*z #define the linear predictor for occurrence
#llambda=llambda+abs(min(llambda))+1
lp<- 0.5+0.5*m # define the linear predictor for detection # was 2
## with negative intercepts qstar is 0.3ish, gets avg detection > 1
# occurrence probability
lambda<-exp(llambda)
# detection probability
p<-exp(lp)/(1+exp(lp))
mean(lambda) ##  2.110151
mean(p) ## 0.6201469
alpha=seq(0,1,by=.05)
alpha=alpha[-1] ## don't scale by zero
# generate presence-absence data
y=lapply(alpha,function(x){rpois(n,(x*lambda)*(1/x*p))})
occurred=lapply(alpha,function(x){rpois(n,x*lambda)})
y2=vector("list",length(alpha))
for(i in 1:length(alpha)){
y2[[i]]=rbinom(n,occurred[[i]],pmin(1/alpha[i]*p,1))
}
summary(y2[[5]])
summary(y2[[10]])
summary(y2[[20]])
summary(y[[1]])
summary(y[[20]])
mean(lambda)
trueAbun
getMeanLambda
head(coeffEst[,c(1:2)])
3.304281+0.4775165*z
summary(models[[20]])
getMeanLambda[20]
i=20
test=matrix(cbind(y[[i]],y2[[i]]),ncol=2)
siteCov=as.data.frame(cbind(z,m))
names(siteCov)=c("z","m")
#obsCov=as.data.frame(matrix(cbind(m,m),ncol=1,byrow=F))
#names(obsCov)="m"
data=unmarkedFramePCount(test,siteCovs=siteCov)
models[[i]]<-pcount(~m ~z,data,mixture="P")
coef(models[[20]])
plogis(coef(models[[20]], type="det")) # Should be close to p
0.04081055 +0.54320381 *z
summary(0.04081055 +0.54320381 *z)
exp(0.04081055 +0.54320381 *z)/(1+exp(0.04081055 +0.54320381 *z))
mean(exp(0.04081055 +0.54320381 *z)/(1+exp(0.04081055 +0.54320381 *z)))
mean(p)
test=rnbinom(n,lambda*p,5)
test
lambda
p
lambda*p
test=rnbinom(n,lambda*p[1],5)
test=rnbinom(n,(lambda*p)[1],5)
test=rnbinom(n,(lambda*p)[1],1)
test
test=rnbinom(n,lambda*p,1)
summary9test
summary(test)
test=rnbinom(n,lambda*p,2)
test=rnbinom(n,lambda*p,.1)
test
summary(test)
test=rnbinom(n,lambda*p,.5)
summary(Test)
summary(test)
summary(y[[20]])
plot(hist(test))
par(mfrow=c(2,1))
plot(hist(test))
par(mfrow=c(2,1))
plot(hist(test))
plot(hist(y[[20]]))
dev.off()
plot(hist(test))
par(mfrow=c(2,1))
plot(hist(test))
dev.off()
par(mfrow=c(2,1))
plot(hist(test))
plot(hist(y[[20]]))
par(mfrow=c(4,1))
plot(hist(test))
plot(hist(y[[20]]))
test2=rnbinom(n,lambda*p,.25)
summary(test2)
plot(test2)
plot(hist(test2))
plot(hist(y[[20]]))
plot(hist(test2))
Sigma <- matrix(c(10,3,3,2),2,2)
require(MASS)
idDist=mvrnorm(n = 1000, rep(0, 2), Sigma)
iidDist=mvrnorm(n = 1000, rep(0, 2), diag(2))
## commonsense check
plot(idDist)
points(iidDist,col="red")
## negative numbers become zero, positive numbers become 1
## map this to bernoulli occurrence?
## each column is a site
require(Matrix)
test=matrix(rnorm(10000),nrow=100)
idDist=mvrnorm(n = 1, rep(0, 100), nearPD(test)$mat)
iidDist=mvrnorm(n = 1, rep(0, 100), diag(100))
idDistBinary=ifelse(idDist>0,1,0)
iidDistBinary=ifelse(iidDist>0,1,0)
table(idDistBinary)
table(iidDistBinary)
#####
set.seed(218116)
n=100
z<- runif(n, -2.5,2.5) # simulate a covariate for occurrence
m<-runif(n,-1,1) # simulate a covariate for detection
lpsi<- 1 +1*z #define the linear predictor for occurrence
lp<- 0.5+0.5*m # define the linear predictor for detection (when these are 2, not a lot of room for detection to be inflated)
###
psi<-exp(lpsi)/(1+exp(lpsi))
# detection probability
p<-exp(lp)/(1+exp(lp))
test=matrix(rnorm(10000,0,.01),nrow=100) ## some trial and error for variance to make sure between 0 and 1
idDist=mvrnorm(n = 1, psi, nearPD(test)$mat)
yID=rbinom(100,1,idDist*p)
yIID=rbinom(100,1,psi*p)
table(yID)
table(yIID)
getLik<-function(y){
lik<-function(parm){
beta0<-parm[1]
beta1<-parm[2]
alpha0<-parm[3]
alpha1<-parm[4]
gridpsi<-
exp(beta0+beta1*z)/(1+exp(beta0+beta1*z))
gridp<-exp(alpha0+alpha1*m)/(1+exp(alpha0+alpha1*m))
q<-gridpsi*gridp
loglik.bin<-sum(y*log(q))+sum((1-y)*log(1-q))
return(-1*loglik.bin)
}
return(lik)
}
likID=getLik(yID)
likIID=getLik(yIID)
optim(rep(0,4),likID)
optim(rep(0,4),likIID)
idDist[which(idDist>1)]=1
occurredID=rbinom(n,1,idDist)
occurredIID=rbinom(n,1,psi)
detectedID=rbinom(n,1,idDist*p)
detectedIID=rbinom(n,1,psi*p)
detectedID2=rbinom(n,1,idDist*p)
detectedIID2=rbinom(n,1,psi*p)
yID=occurredID*detectedID
y2ID=occurredID*detectedID2
yIID=occurredIID*detectedIID
y2IID=occurredIID*detectedIID2
getLik<-function(y,y2){
lik<-function(parm){
beta0<-parm[1]
beta1<-parm[2]
alpha0<-parm[3]
alpha1<-parm[4]
#p<-parm[3]
#idx=parm[4:length(parm)]
#browser()
gridpsi<-
exp(beta0+beta1*z)/(1+exp(beta0+beta1*z))
gridp<-exp(alpha0+alpha1*m)/(1+exp(alpha0+alpha1*m))
q<-gridpsi*gridp
#loglik.bin<-sum(y*log(q))+sum((1-y)*log(1-q))
loglik=sum(y*log(q))+sum((1-y)*log(1-q))+sum((y2*log(q)))+sum(((1-y2)*log(1-q)))
return(-1*loglik)
}
return(lik)
}
likID=getLik(yID,y2ID)
likIID=getLik(yIID,y2IID)
idResults=optim(rep(0.5,4),likID)$par ##  -0.2552096  1.1521820  0.7287478  1.4052447
iidResults=optim(rep(0.5,4),likIID)$par ## 0.7278462 1.2478658 0.2614188 0.7206511
install.packages("mandeR")
require(mandeR)
require(devtools)
install_github('r-barnes/mandeR', vignette=TRUE)
install.packages("mandeR")
install.packages("~/Desktop/mandeR-master.zip", repos = NULL, type = "win.binary")
install_github('gerrymandr/mandeR', vignette=TRUE)
install_github('gerrymandr/mandeR')
baseline=runif(1000)
response=baseline*2+baseline^3*5
baseline=runif(1000)
response=baseline*2+baseline^3*5
plot(baseline,response)
require(purrr)
x <- rerun(5, x = runif(1), y = runif(5))
x
x %>% transpose()
output = x %>% transpose()
output$x[[1]]
output$y[[1]]
shiny::runApp('Desktop/olfactory-expression')
getSimModelCoefficients=function(ballpark,data){
vars=lapply(1:(ncol(data)+1),function(x){seq(-20,20,by=1)})
possBeta=as.matrix(expand.grid(vars))## can take a list
designMat=as.matrix(cbind(rep(1,nrow(data)),data))
## this doesn't work now
avgVal=apply(possBeta,1,function(x){mean(exp(designMat%*%t(x))/(1+exp(designMat%*%t(x))))})
coeffRec= as.matrix(possBeta[which.min(abs(avgVal-ballpark)),])
avgValActual=avgVal[which.min(abs(avgVal-ballpark))]
## probably limitations how feasible this is and how close you can get with lots of covariates
return(list(unname(coeffRec),avgValActual))
}
n=1000
set.seed(1234)
z<- runif(n, -2.5,2.5) # simulate a covariate for occurrence
m<-runif(n,-1,1) # simulate a covariate for detection
data=cbind(z,m)
test=getSimModelCoefficients(0.7,as.data.frame(z)) ## per submodel, not jointly
getSimModelCoefficients=function(ballpark,data){
browser()
vars=lapply(1:(ncol(data)+1),function(x){seq(-20,20,by=1)})
possBeta=as.matrix(expand.grid(vars))## can take a list
designMat=as.matrix(cbind(rep(1,nrow(data)),data))
## this doesn't work now
avgVal=apply(possBeta,1,function(x){mean(exp(designMat%*%t(x))/(1+exp(designMat%*%t(x))))})
coeffRec= as.matrix(possBeta[which.min(abs(avgVal-ballpark)),])
avgValActual=avgVal[which.min(abs(avgVal-ballpark))]
## probably limitations how feasible this is and how close you can get with lots of covariates
return(list(unname(coeffRec),avgValActual))
}
n=1000
set.seed(1234)
z<- runif(n, -2.5,2.5) # simulate a covariate for occurrence
m<-runif(n,-1,1) # simulate a covariate for detection
data=cbind(z,m)
test=getSimModelCoefficients(0.7,as.data.frame(z)) ## per submodel, not jointly
vars=lapply(1:(ncol(data)+1),function(x){seq(-20,20,by=1)})
vars
possBeta=as.matrix(expand.grid(vars))## can take a list
possBeta
designMat=as.matrix(cbind(rep(1,nrow(data)),data))
dim(designMat)
possBeta
possBeta[1,]
dim(designMat)
getSimModelCoefficients=function(ballpark,data){
browser()
vars=lapply(1:(ncol(data)+1),function(x){seq(-20,20,by=1)})
possBeta=as.matrix(expand.grid(vars))## can take a list
designMat=as.matrix(cbind(rep(1,nrow(data)),data))
## this doesn't work now
avgVal=apply(possBeta,1,function(x){mean(exp(designMat%*%as.matrix(x,nrow=ncol(data)+1))/(1+exp(designMat%*%as.matrix(x,nrow=ncol(data)+1))))})
coeffRec= as.matrix(possBeta[which.min(abs(avgVal-ballpark)),])
avgValActual=avgVal[which.min(abs(avgVal-ballpark))]
## probably limitations how feasible this is and how close you can get with lots of covariates
return(list(unname(coeffRec),avgValActual))
}
test=getSimModelCoefficients(0.7,as.data.frame(z)) ## per submodel, not jointly
vars=lapply(1:(ncol(data)+1),function(x){seq(-20,20,by=1)})
possBeta=as.matrix(expand.grid(vars))## can take a list
designMat=as.matrix(cbind(rep(1,nrow(data)),data))
## this doesn't work now
avgVal=apply(possBeta,1,function(x){mean(exp(designMat%*%as.matrix(x,nrow=ncol(data)+1))/(1+exp(designMat%*%as.matrix(x,nrow=ncol(data)+1))))})
coeffRec= as.matrix(possBeta[which.min(abs(avgVal-ballpark)),])
coeffRec
avgValActual=avgVal[which.min(abs(avgVal-ballpark))]
avgValActual
list(unname(coeffRec),avgValActual)
coeffRec
as.vector(coeffRec)
getSimModelCoefficients=function(ballpark,data){
#browser()
vars=lapply(1:(ncol(data)+1),function(x){seq(-20,20,by=1)})
possBeta=as.matrix(expand.grid(vars))## can take a list
designMat=as.matrix(cbind(rep(1,nrow(data)),data))
## this doesn't work now
avgVal=apply(possBeta,1,function(x){mean(exp(designMat%*%as.matrix(x,nrow=ncol(data)+1))/(1+exp(designMat%*%as.matrix(x,nrow=ncol(data)+1))))})
coeffRec= as.matrix(possBeta[which.min(abs(avgVal-ballpark)),])
avgValActual=avgVal[which.min(abs(avgVal-ballpark))]
## probably limitations how feasible this is and how close you can get with lots of covariates
return(list(as.vector(coeffRec),avgValActual))
}
test=getSimModelCoefficients(0.7,as.data.frame(z)) ## per submodel, not jointly
test
require(blogdown)
setwd("~/Desktop/blogdown_source")
blogdown::hugo_build()
blogdown::serve_site()
new_content("blog/2018-08-26-adventures-in-tidyverse-forcats.Rmd")
blogdown::hugo_build()
blogdown::serve_site()
require(blogdown)
setwd("~/Desktop/blogdown_source")
new_content("blog/2018-09-01-adventures-in-tidyverse-reflections.Rmd")
blogdown::hugo_build()
blogdown::serve_site()
require(blogdown)
setwd("~/Desktop/blogdown_source")
new_content("blog/2018-03-29-tools-for-gerrymandering-analysis.Rmd")
blogdown::hugo_build()
blogdown::serve_site()
sys.setenv(GENIUS_API_TOKEN="Vczbo_dkVQ834QXzTZvE5qIKhyANpZwu79ud35uKcJFWXn5pdKlg0GXB9UZD_XCA")
sys.setenv?
Sys.setenv(GENIUS_API_TOKEN="Vczbo_dkVQ834QXzTZvE5qIKhyANpZwu79ud35uKcJFWXn5pdKlg0GXB9UZD_XCA")
require(blogdown)
setwd("~/Desktop/blogdown_source")
Sys.setenv(GENIUS_API_TOKEN="Vczbo_dkVQ834QXzTZvE5qIKhyANpZwu79ud35uKcJFWXn5pdKlg0GXB9UZD_XCA")
new_content("blog/2018-03-27-song-repetition-analysis.Rmd")
blogdown::hugo_build()
blogdown::serve_site()
build_site()
getwd()
getwd()
setwd("~/Desktop/blogdown_source")
new_content("tidytuesday/")
require(blogdown)
new_content("tidytuesday/")
blogdown::hugo_build()
blogdown::serve_site()
new_content("tidytuesday/week3.Rmd")
new_content("tidytuesday/week3.Rmd")
require(blogdown)
getwd()
new_content("tidytuesday/week3.Rmd")
blogdown::hugo_build()
blogdown::serve_site()
require(blogdown)
new_content("blog/week4.Rmd")
blogdown::hugo_build()
blogdown::serve_site()
blogdown::serve_site()
require(blogdown)
new_content("tidytuesday/week5.Rmd")
Sys.setenv(spotify_client_id="e5e1def2dc5245f1b4a2ff06b1067b9a")
Sys.setenv(spotify_client_secret="450e621ac054405d8d1e428cca543f42")
blogdown::hugo_build()
blogdown::serve_site()
blogdown::serve_site()
Sys.getenv(spotify_client_id)
Sys.getenv("spotify_client_id")
blogdown::serve_site()
require(blogdown)
new_content("tidytuesday/week6.Rmd")
blogdown::serve_site()
require(readxl)
require(dplyr)
require(maps)
require(ggmap)
require(fields)
require(USAboundaries)
require(sf)
setwd("~/Desktop/tidytuesday/data/2018-05-07")
coffee1=read_excel("week6_coffee_chains.xlsx",sheet=1)
coffee2=read_excel("week6_coffee_chains.xlsx",sheet=2)
coffee3=read_excel("week6_coffee_chains.xlsx",sheet=3)
## all have different columns
starbucksUS=subset(coffee1,Country=="US")
dunkinUS=subset(coffee3,e_country=="USA")
getMeToCoffee<-function(myPoint,modePrefer,coffeePrefer){
if(is.character(myPoint)){
myPoint2=geocode(myPoint,output="latlon")
}
if(coffeePrefer=="dunkin"){
toDunkin=rdist(as.matrix(myPoint2),dunkinUS[,c("loc_LONG_centroid","loc_LAT_centroid")])
closestDunkin=which.min(toDunkin)
tryThis=paste(dunkinUS$"e_address"[closestDunkin], dunkinUS$e_city[closestDunkin],",",dunkinUS$e_state[closestDunkin],sep=" ")
test= mapdist(myPoint,tryThis, mode = modePrefer,
output = "simple", messaging = FALSE, sensor = FALSE,
language = "en-EN", override_limit = FALSE)
## does better with character addresses than lat longs
}else if(coffeePrefer=="starbucks"){
toStarbucks=rdist(myPoint2,starbucksUS[,c("Longitude","Latitude")])
closestStarbucks=which.min(toStarbucks)
tryThis=paste(starbucksUS[closestStarbucks,"Street Address"], starbucksUS$City[closestStarbucks],",",starbucksUS[closestStarbucks,"State/Province"],sep=" ")
test= mapdist(myPoint,tryThis, mode = modePrefer,
output = "simple", messaging = FALSE, sensor = FALSE,
language = "en-EN", override_limit = FALSE)
}
else{
test=NA
}
return(test)
}
getMeToCoffee("Indiana, PA","driving","dunkin") ## home
getMeToCoffee("100 Bureau Drive, Gaithersburg MD","driving","dunkin") ## NIST address
blogdown::serve_site()
getwd()
setwd("~/Desktop/blogdown_source")
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
require(blogdown)
getwd()
new_content("tidytuesday/week7.Rmd")
blogdown::serve_site()
require(blogdown)
new_content("tidytuesday/week8.Rmd")
blogdown::serve_site()
blogdown::serve_site()
setwd("~/Desktop/tidytuesday/data/2018-05-21/week8_honey_production")
require(dplyr)
#### raw 1 ###
raw1=read.csv("honeyraw_1998to2002.csv",skip=9,header=F,na.strings="",stringsAsFactors=F) ## pick skip by looking manually
raw1=raw1[,3:ncol(raw1)] ## skip first two columns
names(raw1)=c("state","numColonies","yieldPerColony","production","stocks","avgPricePerLb","valProd") ## manually
## pounds, 1000 pounds, 1000 pounds, cents, 1000 dollars
tryThis=raw1[complete.cases(raw1),] ## lots of the weird rows have misisng values
niceData=tryThis[-which(nchar(tryThis$state)>2),] ## just want states
## get year ready
test=niceData %>% group_by(state)%>% summarise(count=n())
test[which(test$count!=5),]
(1998:2002)[ceiling(which(niceData$state=="SC")/nrow(test))]
year=c(rep(1998:2000,each=nrow(test)-1),rep(2001:2002,each=nrow(test)))
length(year)
nrow(niceData)
niceData$year=year
niceData1=niceData
#### raw 2 ####
# skipping other info at top for now
raw2=read.csv("honeyraw_2003to2007.csv",skip=82,header=F,na.strings="",stringsAsFactors=F)
raw2=raw2[,3:ncol(raw2)]
names(raw2)=c("state","numColonies","yieldPerColony","production","stocks","avgPricePerLb","valProd")
tryThis=raw2[complete.cases(raw2),]
niceData=tryThis[-which(nchar(tryThis$state)>2),]
test=niceData %>% group_by(state)%>% summarise(count=n())
test[which(test$count!=5),]
(2003:2007)[ceiling(which(niceData$state=="AL")/nrow(test))]
(2003:2007)[ceiling(which(niceData$state=="MD")/nrow(test))]
(2003:2007)[ceiling(which(niceData$state=="OK")/nrow(test))]
(2003:2007)[ceiling(which(niceData$state=="SC")/nrow(test))]
year=c(rep(2003,nrow(test)),rep(2004:2006,each=nrow(test)-3),rep(2007,nrow(test)-4))
niceData$year=year
niceData2=niceData
#### raw 3 ####
## skip some additional info at top for now
raw3=read.csv("honeyraw_2008to2012.csv",skip=73,header=F,na.strings="",stringsAsFactors=F)
raw3=raw3[,3:ncol(raw3)]
names(raw3)=c("state","numColonies","yieldPerColony","production","stocks","avgPricePerLb","valProd")
## pounds, 1000 pounds, 1000 pounds, cents, 1000 dollars
tryThis=raw3[complete.cases(raw3),]
row.names(tryThis)=NULL
require(readr)
niceData=tryThis[-which(is.na(parse_number(tryThis$numColonies))),]
niceData[which(unlist(lapply(strsplit(niceData$state," "),length))>2),]
niceData=niceData[-which(unlist(lapply(strsplit(niceData$state," "),length))>2),]
test=niceData %>% group_by(state)%>% summarise(count=n())
test[which(test$count!=5),]
which(niceData$state=="Alabama")/nrow(test)
which(niceData$state=="Nevada")/nrow(test)
(2008:2012)[ceiling(which(niceData$state=="Alabama")/nrow(test))]
(2008:2012)[ceiling(which(niceData$state=="Nevada")/nrow(test))]
year=c(rep(2008,nrow(test)),rep(2009:2011,each=nrow(test)-1),rep(2012,nrow(test)-2))
niceData$year=year
niceData3=niceData
## make consistent with other two dataframes
toMatch=cbind.data.frame(abb=state.abb,name=state.name)
niceData3=inner_join(niceData3,toMatch,by=c("state"="name"))
niceData3$state=niceData3$abb
niceData3=niceData3[,-ncol(niceData3)]
head(niceData1)
head(niceData2)
head(niceData3)
niceDataFull=rbind.data.frame(niceData1,niceData2,niceData3)
write.csv(niceDataFull,"honeyDataNice.csv",row.names=F)
getwd()
setwd("~/Desktop/blogdown_source")
blogdown::serve_site()
setwd("~/Desktop/tidytuesday/data/2018-05-21/week8_honey_production")
honey=read.csv("honeyDataNice.csv",stringsAsFactors=F)
names(honey)
byYear=honey %>% group_by(year)%>% summarise(numColoniesT=sum(numColonies),productionT=sum(production),avgPrice=mean(avgPricePerLb),sdPrice=sd(avgPricePerLb),avgYieldPerCol=mean(yieldPerColony),sdYieldPerCol=sd(yieldPerColony),mnumColonies=mean(numColonies),mproduction=mean(production),sdnumColonies=sd(numColonies),sdproduction=sd(production))
blogdown::serve_site()
setwd("~/Desktop/blogdown_source")
blogdown::serve_site()
setwd("~/Desktop/tidytuesday/data/2018-05-21/week8_honey_production")
honey=read.csv("honeyDataNice.csv",stringsAsFactors=F)
names(honey)
byYear=honey %>% group_by(year)%>% summarise(numColoniesT=sum(numColonies),productionT=sum(production),avgPrice=mean(avgPricePerLb),sdPrice=sd(avgPricePerLb),avgYieldPerCol=mean(yieldPerColony),sdYieldPerCol=sd(yieldPerColony),mnumColonies=mean(numColonies),mproduction=mean(production),sdnumColonies=sd(numColonies),sdproduction=sd(production))
ggplot(byYear,aes(x=year,y=mnumColonies))+geom_point()  ## no real difference, scale is narrow
setwd("~/Desktop/blogdown_source")
blogdown::hugo_build()
blogdown::serve_site()
blogdown::serve_site()
require(blogdown)
new_content("tidytuesday/week9.Rmd")
blogdown::serve_site()
blogdown::serve_site()
require(blogdown)
new_content("tidytuesday/week10.Rmd")
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
